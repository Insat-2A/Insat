{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "import keras as kr\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense,LSTM,Embedding,Dropout,Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',-1)\n",
    "df= pd.read_excel(\"text\")\n",
    "df= df.loc[:,\"desc1\":\"Assignment group\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the text using Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[\"desc1\"] = df[\"desc1\"].fillna('').apply(str)\n",
    "df[\"desc1\"] = df[\"desc1\"].str.lower()\n",
    "df[\"desc1\"] = df[\"desc1\"].apply(lambda x: re.sub('http\\S+:\\/\\/.*','',x))\n",
    "df[\"desc1\"] = df[\"desc1\"].apply(lambda x: re.sub('\\w*\\d\\w*\"','',x))\n",
    "df[\"desc1\"] = df[\"desc1\"].str.replace('[^\\w\\s]','')\n",
    "df[\"desc1\"] = df[\"desc1\"].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list2= ['words which you want to remove']\n",
    "list1= list(stopwords.words('english'))\n",
    "list3= list1+list2\n",
    "df[\"description\"]=df[\"description\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (list3)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing un common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq=pd.Series(' '.join(df[\"desc1\"]).split()).value_counts()[-3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"desc1\"] = df[\"desc1\"].apply(lambda x: ' '.join(x for x in x.split() if x not in freq))\n",
    "df[\"desc1\"] = df[\"desc1\"].apply(lambda x:\" \".join(x for x in x.split() if len(x) < 12))\n",
    "df[\"desc1\"] = df[\"desc1\"].apply(lambda x:\" \".join(x for x in x.split() if len(x) > 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words =8000,oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(df[\"desc1\"])\n",
    "word_index = tokenizer.word_index\n",
    "dict1=dict(list(word_index.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Saving the dictinary of tokens in JSON and EXCEL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import json\n",
    "\n",
    "#with open('dict1.json', 'w') as fp:\n",
    "#    json.dump(dict1, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#list1 = [(k, v) for k, v in dict1.items()] \n",
    "#ne=pd.DataFrame(list1)\n",
    "#ne.to_excel(\"C:/Users/PC-ASUS/Desktop/ITIL final files/dictionary.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Loading the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import json\n",
    "#with open('dict1.json', 'r') as fp:\n",
    "#    dict1 = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ne= pd.read_excel(\"C:/Users/PC-ASUS/Desktop/ITIL final files/dictionary.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading the Tokenizer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pk.dump(tokenizer, handle, protocol=pk.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pk.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "dict1=dict(list(word_index.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Text to Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(df[\"desc1\"])\n",
    "X = tokenizer.texts_to_sequences(df[\"desc1\"])\n",
    "X= pad_sequences(X, maxlen=1000, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Encoding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encoding(text):\n",
    "    tokens=kr.preprocessing.text.text_to_word_sequence(text)\n",
    "    tokens=[dict1[word] if word in dict1 else 0 for word in tokens]\n",
    "    return sequence.pad_sequences([tokens],1000,padding=\"post\",truncating=\"post\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reverse_word_index = {value: key for (key, value) in dict1.items()}\n",
    "\n",
    "def decoding(integers):\n",
    "    PAD=0\n",
    "    text=\"\"\n",
    "    for num in integers:\n",
    "        if num != PAD:\n",
    "            text += reverse_word_index[num] + \" \"\n",
    "    return text[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y=le.fit_transform(df[\"Assignment group\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lb=np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "lb=dict(zip(le.classes_, lb))\n",
    "y=tf.keras.utils.to_categorical(y, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape of X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X.shape,y.shape)\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 32)          256000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 64)                16640     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 275,050\n",
      "Trainable params: 275,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(8000, 32),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=.1,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC-ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13717 samples, validate on 5880 samples\n",
      "Epoch 1/7\n",
      " - 856s - loss: 1.3957 - acc: 0.5614 - val_loss: 1.0776 - val_acc: 0.6588\n",
      "Epoch 2/7\n",
      " - 895s - loss: 0.9472 - acc: 0.7006 - val_loss: 0.9509 - val_acc: 0.6906\n",
      "Epoch 3/7\n",
      " - 818s - loss: 0.7802 - acc: 0.7411 - val_loss: 0.8413 - val_acc: 0.7286\n",
      "Epoch 4/7\n",
      " - 820s - loss: 0.6531 - acc: 0.7882 - val_loss: 0.7539 - val_acc: 0.7611\n",
      "Epoch 5/7\n",
      " - 831s - loss: 0.5515 - acc: 0.8242 - val_loss: 0.7350 - val_acc: 0.7670\n",
      "Epoch 6/7\n",
      " - 806s - loss: 0.4665 - acc: 0.8531 - val_loss: 0.7474 - val_acc: 0.7741\n",
      "Epoch 7/7\n",
      " - 809s - loss: 0.4052 - acc: 0.8719 - val_loss: 0.7215 - val_acc: 0.7872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2250f803860>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=32\n",
    "epochs=7\n",
    "model.fit(X_train,y_train,epochs=epochs,batch_size=batch_size,verbose=2, validation_split=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2178/2178 [==============================] - 11s 5ms/step\n",
      "[0.7718274645349977, 0.7699724517906336]\n"
     ]
    }
   ],
   "source": [
    "results= model.evaluate(X_test,y_test)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model(Arch.) and its weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model.save(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_new = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making prediction on new Incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check(text):\n",
    "    encoded_text= encoding(text)\n",
    "    pred = np.zeros((1,1000))\n",
    "    pred[0]=encoded_text\n",
    "    result = model_new.predict(pred)\n",
    "    print (result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "check(\"URL block request\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
